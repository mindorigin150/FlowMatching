{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d49b57-85cb-407c-a2e3-c79f682a3dc1",
   "metadata": {},
   "source": [
    "# Lab One: Simulating ODEs and SDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e99dea-acb4-4fad-a347-1b99567c3188",
   "metadata": {},
   "source": [
    "Welcome to lab one! In this lab, we will provide an intuitive and hands-on walk-through of ODEs and SDEs. If you find any mistakes, or have any other feedback, please feel free to email us at `erives@mit.edu` and `phold@mit.edu`. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe737f00-0dff-4ffe-b40a-8187f8c615b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd11ba4-80a5-4354-913b-ae17827f59e1",
   "metadata": {},
   "source": [
    "# Part 0: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565591-329c-43ba-85fe-620b5eb2219b",
   "metadata": {},
   "source": [
    "First, let us make precise the central objects of study: *ordinary differential equations* (ODEs) and *stochastic differential equations* (SDEs). The basis of both ODEs and SDEs are time-dependent *vector fields*, which we recall from lecture as being functions $u$ defined by $$u:\\mathbb{R}^d\\times [0,1]\\to \\mathbb{R}^d,\\quad (x,t)\\mapsto u_t(x)$$\n",
    "That is, $u_t(x)$ takes in *where in space we are* ($x$) and *where in time we are* ($t$), and spits out the *direction we should be going in* $u_t(x)$. An ODE is then given by $$d X_t = u_t(X_t)dt, \\quad \\quad X_0 = x_0.$$\n",
    "Similarly, an SDE is of the form $$d X_t = u_t(X_t)dt + \\sigma_t d W_t, \\quad \\quad X_0 = x_0,$$\n",
    "which can be thought of as starting with an ODE given by $u_t$, and adding noise via the *Brownian motion* $(W_t)_{0 \\le t \\le 1}$. The amount of noise that we add is given by the *diffusion coefficient* $\\sigma_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738edb40-d990-4ba6-8749-261c66ebe51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class SDE(ABC):\n",
    "    @abstractmethod\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion_coefficient: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02840-39af-41c0-b105-fdcc3aa3d5d2",
   "metadata": {},
   "source": [
    "**Note**: One might consider an ODE to be a special case of SDEs with zero diffusion coefficient. This intuition is valid, however for pedagogical (and performance) reasons, we will treat them separately for the scope of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a220737-5fbb-4405-852d-a9610d27e345",
   "metadata": {},
   "source": [
    "# Part 1: Numerical Methods for Simulating ODEs and SDEs\n",
    "We may think of ODEs and SDEs as describing the motion of a particle through space. Intuitively, the ODE above says \"start at $X_0=x_0$\", and move so that your instantaneous velocity is given by $u_t(X_t)$. Similarly, the SDE says \"start at $X_0=x_0$\", and move so that your instantaneous velocity is given by $u_t(X_t)$ plus a little bit of random noise given scaled by $\\sigma_t$. Formally, these trajectories traced out by this intuitive descriptions are said to be *solutions* to the ODEs and SDEs, respectively. Numerical methods for computing these solutions are all essentially based on *simulating*, or *integrating*, the ODE or SDE. \n",
    "\n",
    "In this section we'll implement the *Euler* and *Euler-Maruyama* numerical simulation schemes for integrating ODEs and SDEs, respectively. Recall from lecture that the Euler simulation scheme corresponds to the discretization\n",
    "\n",
    "$$d X_t = u_t(X_t) dt  \\quad \\quad \\rightarrow \\quad \\quad X_{t + h} = X_t + hu_t(X_t),$$\n",
    "\n",
    "where $h = \\Delta t$ is the *step size*. Similarly, the Euler-Maruyama scheme corresponds to the discretization \n",
    "\n",
    "$$ dX_t = u(X_t,t) dt + \\sigma_t d W_t  \\quad \\quad \\rightarrow \\quad \\quad X_{t + h} = X_t + hu_t(X_t) + \\sqrt{h} \\sigma_t z_t, \\quad z_t \\sim N(0,I_d).$$\n",
    "\n",
    "Let's implement these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98202014-cf8e-4d50-a819-61d937ebea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, dt: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes one simulation step\n",
    "        Args:\n",
    "            - xt: state at time t, shape (batch_size, dim)\n",
    "            - t: time, shape ()\n",
    "            - dt: time, shape ()\n",
    "        Returns:\n",
    "            - nxt: state at time t + dt\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate(self, x: torch.Tensor, ts: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulates using the discretization gives by ts\n",
    "        Args:\n",
    "            - x_init: initial state at time ts[0], shape (batch_size, dim)\n",
    "            - ts: timesteps, shape (nts,)\n",
    "        Returns:\n",
    "            - x_final: final state at time ts[-1], shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        for t_idx in range(len(ts) - 1):\n",
    "            t = ts[t_idx]\n",
    "            h = ts[t_idx + 1] - ts[t_idx]\n",
    "            x = self.step(x, t, h)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate_with_trajectory(self, x: torch.Tensor, ts: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulates using the discretization gives by ts\n",
    "        Args:\n",
    "            - x_init: initial state at time ts[0], shape (bs, dim)\n",
    "            - ts: timesteps, shape (num_timesteps,)\n",
    "        Returns:\n",
    "            - xs: trajectory of xts over ts, shape (batch_size, num_timesteps, dim)\n",
    "        \"\"\"\n",
    "        xs = [x.clone()]\n",
    "        for t_idx in tqdm(range(len(ts) - 1)):\n",
    "            t = ts[t_idx]\n",
    "            h = ts[t_idx + 1] - ts[t_idx]\n",
    "            x = self.step(x, t, h)\n",
    "            xs.append(x.clone())\n",
    "        return torch.stack(xs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b6b5-e7eb-44a1-a180-aa55d16d8356",
   "metadata": {},
   "source": [
    "### Question 1.1: Implement EulerSimulator and EulerMaruyamaSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5b471-a228-4688-903f-7a0898d8b736",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `step` methods of `EulerSimulator` and `EulerMaruyamaSimulator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326c993-b5ef-4b6c-97b5-a403a509b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerSimulator(Simulator):\n",
    "    def __init__(self, ode: ODE):\n",
    "        self.ode = ode\n",
    "        \n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
    "        raise NotImplementedError(\"Fill me in for Question 1.1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fz4dq00ndd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for EulerSimulator\n",
    "def test_euler_simulator():\n",
    "    \"\"\"Test the Euler Simulator implementation\"\"\"\n",
    "    print(\"Testing EulerSimulator...\")\n",
    "    \n",
    "    # Test 1: Create a simple ODE for testing: dx/dt = -x\n",
    "    class SimpleODE(ODE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return -xt\n",
    "    \n",
    "    ode = SimpleODE()\n",
    "    simulator = EulerSimulator(ode)\n",
    "    \n",
    "    xt = torch.tensor([[1.0, 2.0]]).to(device)\n",
    "    t = torch.tensor(0.0).to(device)\n",
    "    h = torch.tensor(0.1).to(device)\n",
    "    \n",
    "    x_next = simulator.step(xt, t, h)\n",
    "    \n",
    "    # Check shape\n",
    "    assert x_next.shape == xt.shape, f\"Shape mismatch: {x_next.shape} vs {xt.shape}\"\n",
    "    print(\"  ✓ Test 1 passed: Basic shape consistency\")\n",
    "    \n",
    "    # Test 2: Check Euler formula: x_{t+h} = x_t + h * drift(x_t, t)\n",
    "    drift = ode.drift_coefficient(xt, t)\n",
    "    expected = xt + h * drift\n",
    "    assert torch.allclose(x_next, expected, atol=1e-6), \\\n",
    "        f\"Euler formula incorrect. Expected {expected}, got {x_next}\"\n",
    "    print(\"  ✓ Test 2 passed: Euler formula correct\")\n",
    "    \n",
    "    # Test 3: Zero drift should not change state\n",
    "    class ZeroODE(ODE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return torch.zeros_like(xt)\n",
    "    \n",
    "    zero_ode = ZeroODE()\n",
    "    zero_simulator = EulerSimulator(zero_ode)\n",
    "    \n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    x_next = zero_simulator.step(xt, t, h)\n",
    "    assert torch.allclose(x_next, xt, atol=1e-6), \\\n",
    "        \"Zero drift should not change state\"\n",
    "    print(\"  ✓ Test 3 passed: Zero drift handling\")\n",
    "    \n",
    "    # Test 4: Batch size consistency\n",
    "    test_batch_sizes = [1, 10, 100]\n",
    "    for bs in test_batch_sizes:\n",
    "        xt = torch.randn(bs, 2).to(device)\n",
    "        x_next = simulator.step(xt, t, h)\n",
    "        assert x_next.shape == (bs, 2), f\"Batch size {bs} failed\"\n",
    "    print(\"  ✓ Test 4 passed: Batch size consistency\")\n",
    "    \n",
    "    # Test 5: Different step sizes\n",
    "    xt = torch.tensor([[1.0, 1.0]]).to(device)\n",
    "    step_sizes = [0.01, 0.1, 0.5, 1.0]\n",
    "    for h_val in step_sizes:\n",
    "        h = torch.tensor(h_val).to(device)\n",
    "        x_next = simulator.step(xt, t, h)\n",
    "        drift = ode.drift_coefficient(xt, t)\n",
    "        expected = xt + h * drift\n",
    "        assert torch.allclose(x_next, expected, atol=1e-6), \\\n",
    "            f\"Step size {h_val} failed\"\n",
    "    print(\"  ✓ Test 5 passed: Different step sizes\")\n",
    "    \n",
    "    # Test 6: Linear ODE with known solution\n",
    "    # For dx/dt = -x, the exact solution is x(t) = x(0) * exp(-t)\n",
    "    class LinearODE(ODE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return -xt\n",
    "    \n",
    "    linear_ode = LinearODE()\n",
    "    linear_simulator = EulerSimulator(linear_ode)\n",
    "    \n",
    "    x0 = torch.tensor([[1.0]]).to(device)\n",
    "    ts = torch.linspace(0.0, 1.0, 101).to(device)  # Small steps for accuracy\n",
    "    x_final = linear_simulator.simulate(x0, ts)\n",
    "    \n",
    "    # Exact solution at t=1\n",
    "    exact = x0 * torch.exp(torch.tensor(-1.0))\n",
    "    \n",
    "    # Euler method should be close with small step size\n",
    "    assert torch.allclose(x_final, exact, atol=0.05), \\\n",
    "        f\"Linear ODE solution incorrect. Expected {exact}, got {x_final}\"\n",
    "    print(\"  ✓ Test 6 passed: Linear ODE with known solution\")\n",
    "    \n",
    "    # Test 7: Time-dependent drift\n",
    "    class TimeDependentODE(ODE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return t * xt  # drift depends on time\n",
    "    \n",
    "    td_ode = TimeDependentODE()\n",
    "    td_simulator = EulerSimulator(td_ode)\n",
    "    \n",
    "    xt = torch.tensor([[1.0, 1.0]]).to(device)\n",
    "    t1 = torch.tensor(0.0).to(device)\n",
    "    t2 = torch.tensor(1.0).to(device)\n",
    "    h = torch.tensor(0.1).to(device)\n",
    "    \n",
    "    x_next1 = td_simulator.step(xt, t1, h)\n",
    "    x_next2 = td_simulator.step(xt, t2, h)\n",
    "    \n",
    "    # Should be different due to time dependence\n",
    "    assert not torch.allclose(x_next1, x_next2, atol=1e-6), \\\n",
    "        \"Time-dependent drift should give different results at different times\"\n",
    "    print(\"  ✓ Test 7 passed: Time-dependent drift\")\n",
    "    \n",
    "    # Test 8: Multi-step simulation\n",
    "    x0 = torch.tensor([[2.0, 3.0]]).to(device)\n",
    "    ts = torch.linspace(0.0, 1.0, 11).to(device)\n",
    "    \n",
    "    x_final = simulator.simulate(x0, ts)\n",
    "    \n",
    "    # Manually compute final state\n",
    "    x_manual = x0.clone()\n",
    "    for i in range(len(ts) - 1):\n",
    "        t_curr = ts[i]\n",
    "        h_curr = ts[i + 1] - ts[i]\n",
    "        x_manual = simulator.step(x_manual, t_curr, h_curr)\n",
    "    \n",
    "    assert torch.allclose(x_final, x_manual, atol=1e-6), \\\n",
    "        \"Multi-step simulation mismatch\"\n",
    "    print(\"  ✓ Test 8 passed: Multi-step simulation\")\n",
    "    \n",
    "    print(\"✅ All EulerSimulator tests passed!\\n\")\n",
    "\n",
    "# Run the tests\n",
    "try:\n",
    "    test_euler_simulator()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed with error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151adb4-e013-4728-8c96-8bab07ee6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerMaruyamaSimulator(Simulator):\n",
    "    def __init__(self, sde: SDE):\n",
    "        self.sde = sde\n",
    "        \n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
    "        raise NotImplementedError(\"Fill me in for Question 1.1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142iirhk34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for EulerMaruyamaSimulator\n",
    "def test_euler_maruyama_simulator():\n",
    "    \"\"\"Test the Euler-Maruyama Simulator implementation\"\"\"\n",
    "    print(\"Testing EulerMaruyamaSimulator...\")\n",
    "    \n",
    "    # Test 1: Create a simple SDE for testing: dx = -x*dt + sigma*dW\n",
    "    class SimpleSDE(SDE):\n",
    "        def __init__(self, sigma):\n",
    "            self.sigma = sigma\n",
    "        \n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return -xt\n",
    "        \n",
    "        def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return torch.ones_like(xt) * self.sigma\n",
    "    \n",
    "    sde = SimpleSDE(sigma=1.0)\n",
    "    simulator = EulerMaruyamaSimulator(sde)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    xt = torch.tensor([[1.0, 2.0]]).to(device)\n",
    "    t = torch.tensor(0.0).to(device)\n",
    "    h = torch.tensor(0.1).to(device)\n",
    "    \n",
    "    x_next = simulator.step(xt, t, h)\n",
    "    \n",
    "    # Check shape\n",
    "    assert x_next.shape == xt.shape, f\"Shape mismatch: {x_next.shape} vs {xt.shape}\"\n",
    "    print(\"  ✓ Test 1 passed: Basic shape consistency\")\n",
    "    \n",
    "    # Test 2: Check Euler-Maruyama formula\n",
    "    # x_{t+h} = x_t + h * drift(x_t, t) + sqrt(h) * diffusion(x_t, t) * z\n",
    "    torch.manual_seed(42)\n",
    "    xt = torch.tensor([[1.0, 2.0]]).to(device)\n",
    "    drift = sde.drift_coefficient(xt, t)\n",
    "    diffusion = sde.diffusion_coefficient(xt, t)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x_next = simulator.step(xt, t, h)\n",
    "    \n",
    "    # The noise should follow the correct pattern\n",
    "    # We can't check exact values due to randomness, but can check it's not deterministic\n",
    "    torch.manual_seed(43)  # Different seed\n",
    "    x_next2 = simulator.step(xt, t, h)\n",
    "    assert not torch.allclose(x_next, x_next2, atol=1e-6), \\\n",
    "        \"Euler-Maruyama should be stochastic\"\n",
    "    print(\"  ✓ Test 2 passed: Stochastic behavior\")\n",
    "    \n",
    "    # Test 3: Zero diffusion should match Euler method\n",
    "    class ZeroDiffusionSDE(SDE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return -xt\n",
    "        \n",
    "        def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return torch.zeros_like(xt)\n",
    "    \n",
    "    zero_sde = ZeroDiffusionSDE()\n",
    "    em_simulator = EulerMaruyamaSimulator(zero_sde)\n",
    "    \n",
    "    # Create matching ODE\n",
    "    class MatchingODE(ODE):\n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return -xt\n",
    "    \n",
    "    euler_simulator = EulerSimulator(MatchingODE())\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    \n",
    "    x_em = em_simulator.step(xt, t, h)\n",
    "    x_euler = euler_simulator.step(xt, t, h)\n",
    "    \n",
    "    assert torch.allclose(x_em, x_euler, atol=1e-6), \\\n",
    "        \"Zero diffusion EM should match Euler method\"\n",
    "    print(\"  ✓ Test 3 passed: Zero diffusion matches Euler\")\n",
    "    \n",
    "    # Test 4: Batch size consistency\n",
    "    test_batch_sizes = [1, 10, 100]\n",
    "    for bs in test_batch_sizes:\n",
    "        torch.manual_seed(42)\n",
    "        xt = torch.randn(bs, 2).to(device)\n",
    "        x_next = simulator.step(xt, t, h)\n",
    "        assert x_next.shape == (bs, 2), f\"Batch size {bs} failed\"\n",
    "    print(\"  ✓ Test 4 passed: Batch size consistency\")\n",
    "    \n",
    "    # Test 5: Noise scaling with sqrt(h)\n",
    "    # Variance should scale with h (not sqrt(h))\n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.zeros(1000, 1).to(device)\n",
    "    \n",
    "    # Small step\n",
    "    h_small = torch.tensor(0.01).to(device)\n",
    "    torch.manual_seed(42)\n",
    "    x_small = simulator.step(x0, t, h_small)\n",
    "    var_small = x_small.var()\n",
    "    \n",
    "    # Larger step (4x)\n",
    "    h_large = torch.tensor(0.04).to(device)\n",
    "    torch.manual_seed(42)\n",
    "    x_large = simulator.step(x0, t, h_large)\n",
    "    var_large = x_large.var()\n",
    "    \n",
    "    # Variance should scale linearly with h for pure diffusion\n",
    "    # var_large / var_small should be close to h_large / h_small = 4\n",
    "    ratio = var_large / var_small\n",
    "    assert 3.0 < ratio < 5.0, \\\n",
    "        f\"Variance scaling incorrect: {ratio} (expected ~4.0)\"\n",
    "    print(\"  ✓ Test 5 passed: Noise scaling with sqrt(h)\")\n",
    "    \n",
    "    # Test 6: Brownian motion asymptotic behavior\n",
    "    # For dX = sigma * dW, X_t ~ N(0, sigma^2 * t)\n",
    "    class PureBrownian(SDE):\n",
    "        def __init__(self, sigma):\n",
    "            self.sigma = sigma\n",
    "        \n",
    "        def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return torch.zeros_like(xt)\n",
    "        \n",
    "        def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "            return torch.ones_like(xt) * self.sigma\n",
    "    \n",
    "    sigma = 2.0\n",
    "    brownian = PureBrownian(sigma)\n",
    "    brownian_sim = EulerMaruyamaSimulator(brownian)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.zeros(1000, 1).to(device)\n",
    "    T = 1.0\n",
    "    ts = torch.linspace(0.0, T, 101).to(device)\n",
    "    x_final = brownian_sim.simulate(x0, ts)\n",
    "    \n",
    "    # Should have mean ~0 and variance ~sigma^2 * T\n",
    "    mean = x_final.mean()\n",
    "    var = x_final.var()\n",
    "    expected_var = sigma**2 * T\n",
    "    \n",
    "    assert abs(mean) < 0.2, f\"Mean should be ~0, got {mean}\"\n",
    "    assert abs(var - expected_var) < 1.0, \\\n",
    "        f\"Variance should be ~{expected_var}, got {var}\"\n",
    "    print(\"  ✓ Test 6 passed: Brownian motion statistics\")\n",
    "    \n",
    "    print(\"✅ All EulerMaruyamaSimulator tests passed!\\n\")\n",
    "\n",
    "# Run the tests\n",
    "try:\n",
    "    test_euler_maruyama_simulator()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed with error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb930b6-63a1-4cc2-8f60-d30de5d380ea",
   "metadata": {},
   "source": [
    "**Note:** When the diffusion coefficient is zero, the Euler and Euler-Maruyama simulation are equivalent! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdf59a-25e3-4537-a357-bbbb8ee5332b",
   "metadata": {},
   "source": [
    "# Part 2: Visualizing Solutions to SDEs\n",
    "Let's get a feel for what the solutions to these SDEs look like in practice (we'll get to ODEs later...). To do so, we we'll implement and visualize two special choices of SDEs from lecture: a (scaled) *Brownian motion*, and an *Ornstein-Uhlenbeck* (OU) process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c480e6-bfcf-4105-b21a-002d3a110923",
   "metadata": {},
   "source": [
    "### Question 2.1: Implementing Brownian Motion\n",
    "First, recall that a Brownian motion is recovered (by definition) by setting $u_t = 0$ and $\\sigma_t = \\sigma$, viz.,\n",
    "$$ dX_t = \\sigma dW_t, \\quad \\quad X_0 = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e83673-54dc-451e-8422-0110c462b295",
   "metadata": {},
   "source": [
    "**Your job**: Intuitively, what might be expect the trajectories of $X_t$ to look like when $\\sigma$ is very large? What about when $\\sigma$ is close to zero?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba5c9e-f5cc-41a9-a850-43e46d79b3fb",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `drift_coefficient` and `difusion_coefficient` methods of the `BrownianMotion` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372a79-585e-4bbb-a78e-3870dd5c458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownianMotion(SDE):\n",
    "    def __init__(self, sigma: float):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.1!\")\n",
    "        \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5z6xh4lnu4h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for BrownianMotion\n",
    "def test_brownian_motion():\n",
    "    \"\"\"Test the Brownian Motion implementation\"\"\"\n",
    "    print(\"Testing BrownianMotion...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    sigma = 1.0\n",
    "    brownian = BrownianMotion(sigma)\n",
    "    \n",
    "    xt = torch.tensor([[1.0, 2.0], [3.0, 4.0]]).to(device)\n",
    "    t = torch.tensor(0.5).to(device)\n",
    "    \n",
    "    drift = brownian.drift_coefficient(xt, t)\n",
    "    diffusion = brownian.diffusion_coefficient(xt, t)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert drift.shape == xt.shape, f\"Drift shape mismatch: {drift.shape} vs {xt.shape}\"\n",
    "    assert diffusion.shape == xt.shape, f\"Diffusion shape mismatch\"\n",
    "    print(\"  ✓ Test 1 passed: Basic shape consistency\")\n",
    "    \n",
    "    # Test 2: Drift should be zero (definition of Brownian motion)\n",
    "    expected_drift = torch.zeros_like(xt)\n",
    "    assert torch.allclose(drift, expected_drift, atol=1e-6), \\\n",
    "        f\"Drift should be zero, got {drift}\"\n",
    "    print(\"  ✓ Test 2 passed: Drift coefficient is zero\")\n",
    "    \n",
    "    # Test 3: Diffusion should be sigma\n",
    "    expected_diffusion = torch.ones_like(xt) * sigma\n",
    "    assert torch.allclose(diffusion, expected_diffusion, atol=1e-6), \\\n",
    "        f\"Diffusion should be {sigma}, got {diffusion}\"\n",
    "    print(\"  ✓ Test 3 passed: Diffusion coefficient is sigma\")\n",
    "    \n",
    "    # Test 4: Different batch sizes\n",
    "    test_batch_sizes = [1, 10, 100]\n",
    "    for bs in test_batch_sizes:\n",
    "        xt = torch.randn(bs, 2).to(device)\n",
    "        drift = brownian.drift_coefficient(xt, t)\n",
    "        diffusion = brownian.diffusion_coefficient(xt, t)\n",
    "        assert drift.shape == (bs, 2), f\"Drift batch size {bs} failed\"\n",
    "        assert diffusion.shape == (bs, 2), f\"Diffusion batch size {bs} failed\"\n",
    "        assert torch.allclose(drift, torch.zeros_like(xt), atol=1e-6), \\\n",
    "            f\"Drift should be zero for batch size {bs}\"\n",
    "    print(\"  ✓ Test 4 passed: Batch size consistency\")\n",
    "    \n",
    "    # Test 5: Time independence\n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    t1 = torch.tensor(0.0).to(device)\n",
    "    t2 = torch.tensor(1.0).to(device)\n",
    "    \n",
    "    drift1 = brownian.drift_coefficient(xt, t1)\n",
    "    drift2 = brownian.drift_coefficient(xt, t2)\n",
    "    diffusion1 = brownian.diffusion_coefficient(xt, t1)\n",
    "    diffusion2 = brownian.diffusion_coefficient(xt, t2)\n",
    "    \n",
    "    assert torch.allclose(drift1, drift2, atol=1e-6), \"Drift should be time-independent\"\n",
    "    assert torch.allclose(diffusion1, diffusion2, atol=1e-6), \\\n",
    "        \"Diffusion should be time-independent\"\n",
    "    print(\"  ✓ Test 5 passed: Time independence\")\n",
    "    \n",
    "    # Test 6: Different sigma values\n",
    "    sigmas = [0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    for sig in sigmas:\n",
    "        bm = BrownianMotion(sig)\n",
    "        diffusion = bm.diffusion_coefficient(xt, t)\n",
    "        expected = torch.ones_like(xt) * sig\n",
    "        assert torch.allclose(diffusion, expected, atol=1e-6), \\\n",
    "            f\"Diffusion incorrect for sigma={sig}\"\n",
    "    print(\"  ✓ Test 6 passed: Different sigma values\")\n",
    "    \n",
    "    # Test 7: Integration with EulerMaruyamaSimulator\n",
    "    sigma = 2.0\n",
    "    brownian = BrownianMotion(sigma)\n",
    "    simulator = EulerMaruyamaSimulator(brownian)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.zeros(1000, 1).to(device)\n",
    "    T = 1.0\n",
    "    ts = torch.linspace(0.0, T, 101).to(device)\n",
    "    x_final = simulator.simulate(x0, ts)\n",
    "    \n",
    "    # For Brownian motion: X_t ~ N(0, sigma^2 * t)\n",
    "    mean = x_final.mean()\n",
    "    var = x_final.var()\n",
    "    expected_var = sigma**2 * T\n",
    "    \n",
    "    assert abs(mean) < 0.2, f\"Mean should be ~0, got {mean}\"\n",
    "    assert abs(var - expected_var) < 1.5, \\\n",
    "        f\"Variance should be ~{expected_var}, got {var}\"\n",
    "    print(\"  ✓ Test 7 passed: Integration with simulator\")\n",
    "    \n",
    "    # Test 8: State independence (drift and diffusion don't depend on x)\n",
    "    xt1 = torch.tensor([[0.0, 0.0]]).to(device)\n",
    "    xt2 = torch.tensor([[10.0, -10.0]]).to(device)\n",
    "    \n",
    "    drift1 = brownian.drift_coefficient(xt1, t)\n",
    "    drift2 = brownian.drift_coefficient(xt2, t)\n",
    "    diffusion1 = brownian.diffusion_coefficient(xt1, t)\n",
    "    diffusion2 = brownian.diffusion_coefficient(xt2, t)\n",
    "    \n",
    "    assert torch.allclose(drift1, drift2, atol=1e-6), \\\n",
    "        \"Drift should not depend on state\"\n",
    "    assert torch.allclose(diffusion1, diffusion2, atol=1e-6), \\\n",
    "        \"Diffusion should not depend on state\"\n",
    "    print(\"  ✓ Test 8 passed: State independence\")\n",
    "    \n",
    "    print(\"✅ All BrownianMotion tests passed!\\n\")\n",
    "\n",
    "# Run the tests\n",
    "try:\n",
    "    test_brownian_motion()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed with error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2318e-ae09-426a-bf57-c44cd5074288",
   "metadata": {},
   "source": [
    "Now let's plot! We'll make use of the following utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577649b-2290-4308-9d5a-7ab614984b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_1d(x0: torch.Tensor, simulator: Simulator, timesteps: torch.Tensor, ax: Optional[Axes] = None):\n",
    "        \"\"\"\n",
    "        Graphs the trajectories of a one-dimensional SDE with given initial values (x0) and simulation timesteps (timesteps).\n",
    "        Args:\n",
    "            - x0: state at time t, shape (num_trajectories, 1)\n",
    "            - simulator: Simulator object used to simulate\n",
    "            - t: timesteps to simulate along, shape (num_timesteps,)\n",
    "            - ax: pyplot Axes object to plot on\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        trajectories = simulator.simulate_with_trajectory(x0, timesteps) # (num_trajectories, num_timesteps, ...)\n",
    "        for trajectory_idx in range(trajectories.shape[0]):\n",
    "            trajectory = trajectories[trajectory_idx, :, 0] # (num_timesteps,)\n",
    "            ax.plot(ts.cpu(), trajectory.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4245ac9-3092-4c5b-a149-75a909ceaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "brownian_motion = BrownianMotion(sigma)\n",
    "simulator = EulerMaruyamaSimulator(sde=brownian_motion)\n",
    "x0 = torch.zeros(5,1).to(device) # Initial values - let's start at zero\n",
    "ts = torch.linspace(0.0,5.0,500).to(device) # simulation timesteps\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "ax.set_title(r'Trajectories of Brownian Motion with $\\sigma=$' + str(sigma), fontsize=18)\n",
    "ax.set_xlabel(r'Time ($t$)', fontsize=18)\n",
    "ax.set_ylabel(r'$X_t$', fontsize=18)\n",
    "plot_trajectories_1d(x0, simulator, ts, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7bd5c-d558-45e0-b741-6de4d7003776",
   "metadata": {},
   "source": [
    "**Your job**: What happens when you vary the value of `sigma`?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22c81b-7ce9-4b84-82ab-fac03f741b03",
   "metadata": {},
   "source": [
    "### Question 2.2: Implementing an Ornstein-Uhlenbeck Process\n",
    "An OU process is given by setting $u_t(X_t) = - \\theta X_t$ and $\\sigma_t = \\sigma$, viz.,\n",
    "$$ dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cdb7d-f8bf-4826-9c32-8cb101376103",
   "metadata": {},
   "source": [
    "**Your job**: Intuitively, what would the trajectory of $X_t$ look like for a very small value of $\\theta$? What about a very large value of $\\theta$?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12325951-709c-4486-9ea7-f4c22b3cc1ef",
   "metadata": {},
   "source": [
    "**Your job**: Fill in the `drift_coefficient` and `difusion_coefficient` methods of the `OUProcess` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d0e94-698c-4729-878e-3c2a9881dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUProcess(SDE):\n",
    "    def __init__(self, theta: float, sigma: float):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the drift coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - drift: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.2!\")\n",
    "        \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the diffusion coefficient of the ODE.\n",
    "        Args:\n",
    "            - xt: state at time t, shape (bs, dim)\n",
    "            - t: time, shape ()\n",
    "        Returns:\n",
    "            - diffusion: shape (bs, dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Fill me in for Question 2.2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hodqvowo1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for OUProcess\n",
    "def test_ou_process():\n",
    "    \"\"\"Test the Ornstein-Uhlenbeck Process implementation\"\"\"\n",
    "    print(\"Testing OUProcess...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    theta = 0.5\n",
    "    sigma = 1.0\n",
    "    ou = OUProcess(theta, sigma)\n",
    "    \n",
    "    xt = torch.tensor([[1.0, 2.0], [3.0, 4.0]]).to(device)\n",
    "    t = torch.tensor(0.5).to(device)\n",
    "    \n",
    "    drift = ou.drift_coefficient(xt, t)\n",
    "    diffusion = ou.diffusion_coefficient(xt, t)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert drift.shape == xt.shape, f\"Drift shape mismatch: {drift.shape} vs {xt.shape}\"\n",
    "    assert diffusion.shape == xt.shape, f\"Diffusion shape mismatch\"\n",
    "    print(\"  ✓ Test 1 passed: Basic shape consistency\")\n",
    "    \n",
    "    # Test 2: Drift should be -theta * x\n",
    "    expected_drift = -theta * xt\n",
    "    assert torch.allclose(drift, expected_drift, atol=1e-6), \\\n",
    "        f\"Drift should be -theta*x. Expected {expected_drift}, got {drift}\"\n",
    "    print(\"  ✓ Test 2 passed: Drift coefficient is -theta * x\")\n",
    "    \n",
    "    # Test 3: Diffusion should be sigma\n",
    "    expected_diffusion = torch.ones_like(xt) * sigma\n",
    "    assert torch.allclose(diffusion, expected_diffusion, atol=1e-6), \\\n",
    "        f\"Diffusion should be {sigma}, got {diffusion}\"\n",
    "    print(\"  ✓ Test 3 passed: Diffusion coefficient is sigma\")\n",
    "    \n",
    "    # Test 4: Equilibrium property (drift at origin is zero)\n",
    "    xt_zero = torch.zeros(10, 2).to(device)\n",
    "    drift_zero = ou.drift_coefficient(xt_zero, t)\n",
    "    assert torch.allclose(drift_zero, torch.zeros_like(xt_zero), atol=1e-6), \\\n",
    "        \"Drift at origin should be zero\"\n",
    "    print(\"  ✓ Test 4 passed: Drift at origin is zero\")\n",
    "    \n",
    "    # Test 5: Mean reversion (drift points toward origin)\n",
    "    xt_positive = torch.ones(10, 2).to(device) * 5.0\n",
    "    drift_positive = ou.drift_coefficient(xt_positive, t)\n",
    "    # Drift should be negative (toward origin)\n",
    "    assert (drift_positive < 0).all(), \"Drift should point toward origin for positive x\"\n",
    "    \n",
    "    xt_negative = torch.ones(10, 2).to(device) * -5.0\n",
    "    drift_negative = ou.drift_coefficient(xt_negative, t)\n",
    "    # Drift should be positive (toward origin)\n",
    "    assert (drift_negative > 0).all(), \"Drift should point toward origin for negative x\"\n",
    "    print(\"  ✓ Test 5 passed: Mean reversion property\")\n",
    "    \n",
    "    # Test 6: Different theta and sigma values\n",
    "    test_params = [(0.1, 0.5), (0.5, 1.0), (1.0, 2.0), (5.0, 0.5)]\n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    for th, sig in test_params:\n",
    "        ou_test = OUProcess(th, sig)\n",
    "        drift = ou_test.drift_coefficient(xt, t)\n",
    "        diffusion = ou_test.diffusion_coefficient(xt, t)\n",
    "        \n",
    "        expected_drift = -th * xt\n",
    "        expected_diffusion = torch.ones_like(xt) * sig\n",
    "        \n",
    "        assert torch.allclose(drift, expected_drift, atol=1e-6), \\\n",
    "            f\"Drift incorrect for theta={th}, sigma={sig}\"\n",
    "        assert torch.allclose(diffusion, expected_diffusion, atol=1e-6), \\\n",
    "            f\"Diffusion incorrect for theta={th}, sigma={sig}\"\n",
    "    print(\"  ✓ Test 6 passed: Different parameter values\")\n",
    "    \n",
    "    # Test 7: Batch size consistency\n",
    "    test_batch_sizes = [1, 10, 100]\n",
    "    for bs in test_batch_sizes:\n",
    "        xt = torch.randn(bs, 2).to(device)\n",
    "        drift = ou.drift_coefficient(xt, t)\n",
    "        diffusion = ou.diffusion_coefficient(xt, t)\n",
    "        assert drift.shape == (bs, 2), f\"Drift batch size {bs} failed\"\n",
    "        assert diffusion.shape == (bs, 2), f\"Diffusion batch size {bs} failed\"\n",
    "    print(\"  ✓ Test 7 passed: Batch size consistency\")\n",
    "    \n",
    "    # Test 8: Time independence\n",
    "    xt = torch.randn(10, 2).to(device)\n",
    "    t1 = torch.tensor(0.0).to(device)\n",
    "    t2 = torch.tensor(1.0).to(device)\n",
    "    \n",
    "    drift1 = ou.drift_coefficient(xt, t1)\n",
    "    drift2 = ou.drift_coefficient(xt, t2)\n",
    "    diffusion1 = ou.diffusion_coefficient(xt, t1)\n",
    "    diffusion2 = ou.diffusion_coefficient(xt, t2)\n",
    "    \n",
    "    assert torch.allclose(drift1, drift2, atol=1e-6), \"Drift should be time-independent\"\n",
    "    assert torch.allclose(diffusion1, diffusion2, atol=1e-6), \\\n",
    "        \"Diffusion should be time-independent\"\n",
    "    print(\"  ✓ Test 8 passed: Time independence\")\n",
    "    \n",
    "    # Test 9: Long-time behavior (convergence to stationary distribution)\n",
    "    # For OU process: stationary distribution is N(0, sigma^2/(2*theta))\n",
    "    theta = 1.0\n",
    "    sigma = 2.0\n",
    "    ou = OUProcess(theta, sigma)\n",
    "    simulator = EulerMaruyamaSimulator(ou)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.ones(1000, 1).to(device) * 10.0  # Start far from equilibrium\n",
    "    T = 20.0  # Long time\n",
    "    ts = torch.linspace(0.0, T, 2001).to(device)\n",
    "    x_final = simulator.simulate(x0, ts)\n",
    "    \n",
    "    # Stationary distribution: N(0, sigma^2/(2*theta))\n",
    "    expected_mean = 0.0\n",
    "    expected_var = sigma**2 / (2 * theta)\n",
    "    \n",
    "    mean = x_final.mean()\n",
    "    var = x_final.var()\n",
    "    \n",
    "    assert abs(mean - expected_mean) < 0.3, \\\n",
    "        f\"Mean should converge to {expected_mean}, got {mean}\"\n",
    "    assert abs(var - expected_var) < 1.0, \\\n",
    "        f\"Variance should converge to {expected_var}, got {var}\"\n",
    "    print(\"  ✓ Test 9 passed: Long-time stationary distribution\")\n",
    "    \n",
    "    # Test 10: Stronger mean reversion with larger theta\n",
    "    theta_weak = 0.1\n",
    "    theta_strong = 2.0\n",
    "    sigma = 1.0\n",
    "    \n",
    "    ou_weak = OUProcess(theta_weak, sigma)\n",
    "    ou_strong = OUProcess(theta_strong, sigma)\n",
    "    \n",
    "    sim_weak = EulerMaruyamaSimulator(ou_weak)\n",
    "    sim_strong = EulerMaruyamaSimulator(ou_strong)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.ones(100, 1).to(device) * 5.0\n",
    "    ts = torch.linspace(0.0, 2.0, 201).to(device)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x_weak = sim_weak.simulate(x0, ts)\n",
    "    torch.manual_seed(42)\n",
    "    x_strong = sim_strong.simulate(x0, ts)\n",
    "    \n",
    "    # Stronger theta should pull back to origin faster\n",
    "    assert x_strong.abs().mean() < x_weak.abs().mean(), \\\n",
    "        \"Stronger theta should lead to faster mean reversion\"\n",
    "    print(\"  ✓ Test 10 passed: Stronger theta leads to faster mean reversion\")\n",
    "    \n",
    "    print(\"✅ All OUProcess tests passed!\\n\")\n",
    "\n",
    "# Run the tests\n",
    "try:\n",
    "    test_ou_process()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed with error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dc249-826e-4a81-91f7-fa8fb17278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try comparing multiple choices side-by-side\n",
    "thetas_and_sigmas = [\n",
    "    (0.25, 0.0),\n",
    "    (0.25, 0.25),\n",
    "    (0.25, 0.5),\n",
    "    (0.25, 1.0),\n",
    "]\n",
    "simulation_time = 20.0\n",
    "\n",
    "num_plots = len(thetas_and_sigmas)\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(8 * num_plots, 7))\n",
    "\n",
    "for idx, (theta, sigma) in enumerate(thetas_and_sigmas):\n",
    "    ou_process = OUProcess(theta, sigma)\n",
    "    simulator = EulerMaruyamaSimulator(sde=ou_process)\n",
    "    x0 = torch.linspace(-10.0,10.0,10).view(-1,1).to(device) # Initial values - let's start at zero\n",
    "    ts = torch.linspace(0.0,simulation_time,1000).to(device) # simulation timesteps\n",
    "\n",
    "    ax = axes[idx]\n",
    "    ax.set_title(f'Trajectories of OU Process with $\\\\sigma = ${sigma}, $\\\\theta = ${theta}', fontsize=15)\n",
    "    ax.set_xlabel(r'Time ($t$)', fontsize=15)\n",
    "    ax.set_ylabel(r'$X_t$', fontsize=15)\n",
    "    plot_trajectories_1d(x0, simulator, ts, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0c7eb-0eb3-4eb3-a854-5cac18a85baa",
   "metadata": {},
   "source": [
    "**Your job**: What do you notice about the convergence of the solutions? Are they converging to a particular point? Or to a distribution? Your answer should be two *qualitative* sentences of the form: \"When ($\\theta$ or $\\sigma$) goes (up or down), we see...\".\n",
    "\n",
    "**Hint**: Pay close attention to the ratio $D \\triangleq \\frac{\\sigma^2}{2\\theta}$ (see the next few cells below!).\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31502fba-c582-4d67-8fdf-8d474604cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scaled_trajectories_1d(x0: torch.Tensor, simulator: Simulator, timesteps: torch.Tensor, time_scale: float, label: str, ax: Optional[Axes] = None):\n",
    "        \"\"\"\n",
    "        Graphs the trajectories of a one-dimensional SDE with given initial values (x0) and simulation timesteps (timesteps).\n",
    "        Args:\n",
    "            - x0: state at time t, shape (num_trajectories, 1)\n",
    "            - simulator: Simulator object used to simulate\n",
    "            - t: timesteps to simulate along, shape (num_timesteps,)\n",
    "            - time_scale: scalar by which to scale time\n",
    "            - label: self-explanatory\n",
    "            - ax: pyplot Axes object to plot on\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        trajectories = simulator.simulate_with_trajectory(x0, timesteps) # (num_trajectories, num_timesteps, ...)\n",
    "        for trajectory_idx in range(trajectories.shape[0]):\n",
    "            trajectory = trajectories[trajectory_idx, :, 0] # (num_timesteps,)\n",
    "            ax.plot(ts.cpu() * time_scale, trajectory.cpu(), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c533-a8cf-4fc0-be0c-b5fd5c3cbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try rescaling with time\n",
    "sigmas = [1.0, 2.0, 10.0]\n",
    "ds = [0.25, 1.0, 4.0] # sigma**2 / 2t\n",
    "simulation_time = 10.0\n",
    "\n",
    "fig, axes = plt.subplots(len(ds), len(sigmas), figsize=(8 * len(sigmas), 8 * len(ds)))\n",
    "axes = axes.reshape((len(ds), len(sigmas)))\n",
    "for d_idx, d in enumerate(ds):\n",
    "    for s_idx, sigma in enumerate(sigmas):\n",
    "        theta = sigma**2 / 2 / d\n",
    "        ou_process = OUProcess(theta, sigma)\n",
    "        simulator = EulerMaruyamaSimulator(sde=ou_process)\n",
    "        x0 = torch.linspace(-20.0,20.0,20).view(-1,1).to(device)\n",
    "        time_scale = sigma**2\n",
    "        ts = torch.linspace(0.0,simulation_time / time_scale,1000).to(device) # simulation timesteps\n",
    "        ax = axes[d_idx, s_idx]\n",
    "        plot_scaled_trajectories_1d(x0=x0, simulator=simulator, timesteps=ts, time_scale=time_scale, label=f'Sigma = {sigma}', ax=ax)\n",
    "        ax.set_title(f'OU Trajectories with Sigma={sigma}, Theta={theta}, D={d}')\n",
    "        ax.set_xlabel(f't / (sigma^2)')\n",
    "        ax.set_ylabel('X_t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850111a6-30be-4265-b423-ed23671deaf0",
   "metadata": {},
   "source": [
    "**Your job**: What conclusion can we draw from the figure above? One qualitative sentence is fine. We'll revisit this in Section 3.2.\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bcbdc-237c-4639-a447-b0c13f575a8d",
   "metadata": {},
   "source": [
    "# Part 3: Transforming Distributions with SDEs\n",
    "In the previous section, we observed how individual *points* are transformed by an SDE. Ultimately, we are interested in understanding how *distributions* are transformed by an SDE (or an ODE...). After all, our goal is to design ODEs and SDEs which transform a noisy distribution (such as the Gaussian $N(0, I_d)$), to the data distribution $p_{\\text{data}}$ of interest. In this section, we will visualize how distributions are transformed by a very particular family of SDEs: *Langevin dynamics*.\n",
    "\n",
    "First, let's define some distributions to play around with. In practice, there are two qualities one might hope a distribution to have:\n",
    "1. The first quality is that one can measure the *density* of a distribution $p(x)$. This ensures that we can compute the gradient $\\nabla \\log p(x)$ of the log density. This quantity is known as the *score* of $p$, and paints a picture of the local geometry of the distribution. Using the score, we will construct and simulate the *Langevin dynamics*, a family of SDEs which \"drive\" samples toward the distribution $\\pi$. In particular, the Langevin dynamics *preserve* the distribution $p(x)$. In Lecture 2, we will make this notion of driving more precise.\n",
    "2. The second quality is that we can draw samples from the distribution $p(x)$.\n",
    "For simple, toy distributions, such as Gaussians and simple mixture models, it is often true that both qualities are satisfied. For more complex choices of $p$, such as distributions over images, we can sample but cannot measure the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b64f2-732a-4ea3-84a4-8f955ca64f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Density(ABC):\n",
    "    \"\"\"\n",
    "    Distribution with tractable density\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def log_density(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the log density at x.\n",
    "        Args:\n",
    "            - x: shape (batch_size, dim)\n",
    "        Returns:\n",
    "            - log_density: shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def score(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the score dx log density(x)\n",
    "        Args:\n",
    "            - x: (batch_size, dim)\n",
    "        Returns:\n",
    "            - score: (batch_size, dim)\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, ...)\n",
    "        score = vmap(jacrev(self.log_density))(x)  # (batch_size, 1, 1, 1, ...)\n",
    "        return score.squeeze((1, 2, 3))  # (batch_size, ...)\n",
    "\n",
    "class Sampleable(ABC):\n",
    "    \"\"\"\n",
    "    Distribution which can be sampled from\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the log density at x.\n",
    "        Args:\n",
    "            - num_samples: the desired number of samples\n",
    "        Returns:\n",
    "            - samples: shape (batch_size, dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805b3f8-f0ab-4bb0-a41a-4d97c65e24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several plotting utility functions\n",
    "def hist2d_sampleable(sampleable: Sampleable, num_samples: int, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    samples = sampleable.sample(num_samples) # (ns, 2)\n",
    "    ax.hist2d(samples[:,0].cpu(), samples[:,1].cpu(), **kwargs)\n",
    "\n",
    "def scatter_sampleable(sampleable: Sampleable, num_samples: int, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    samples = sampleable.sample(num_samples) # (ns, 2)\n",
    "    ax.scatter(samples[:,0].cpu(), samples[:,1].cpu(), **kwargs)\n",
    "\n",
    "def imshow_density(density: Density, bins: int, scale: float, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = torch.linspace(-scale, scale, bins).to(device)\n",
    "    y = torch.linspace(-scale, scale, bins).to(device)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    xy = torch.stack([X.reshape(-1), Y.reshape(-1)], dim=-1)\n",
    "    density = density.log_density(xy).reshape(bins, bins).T\n",
    "    im = ax.imshow(density.cpu(), extent=[-scale, scale, -scale, scale], origin='lower', **kwargs)\n",
    "\n",
    "def contour_density(density: Density, bins: int, scale: float, ax: Optional[Axes] = None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = torch.linspace(-scale, scale, bins).to(device)\n",
    "    y = torch.linspace(-scale, scale, bins).to(device)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    xy = torch.stack([X.reshape(-1), Y.reshape(-1)], dim=-1)\n",
    "    density = density.log_density(xy).reshape(bins, bins).T\n",
    "    im = ax.contour(density.cpu(), extent=[-scale, scale, -scale, scale], origin='lower', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eb6cb-1261-4cc1-b1d0-281e5f73d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(torch.nn.Module, Sampleable, Density):\n",
    "    \"\"\"\n",
    "    Two-dimensional Gaussian. Is a Density and a Sampleable. Wrapper around torch.distributions.MultivariateNormal\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, cov):\n",
    "        \"\"\"\n",
    "        mean: shape (2,)\n",
    "        cov: shape (2,2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"cov\", cov)\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MultivariateNormal(self.mean, self.cov, validate_args=False)\n",
    "\n",
    "    def sample(self, num_samples) -> torch.Tensor:\n",
    "        return self.distribution.sample((num_samples,))\n",
    "\n",
    "    def log_density(self, x: torch.Tensor):\n",
    "        return self.distribution.log_prob(x).view(-1, 1)\n",
    "\n",
    "class GaussianMixture(torch.nn.Module, Sampleable, Density):\n",
    "    \"\"\"\n",
    "    Two-dimensional Gaussian mixture model, and is a Density and a Sampleable. Wrapper around torch.distributions.MixtureSameFamily.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        means: torch.Tensor,  # nmodes x data_dim\n",
    "        covs: torch.Tensor,  # nmodes x data_dim x data_dim\n",
    "        weights: torch.Tensor,  # nmodes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        means: shape (nmodes, 2)\n",
    "        covs: shape (nmodes, 2, 2)\n",
    "        weights: shape (nmodes, 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nmodes = means.shape[0]\n",
    "        self.register_buffer(\"means\", means)\n",
    "        self.register_buffer(\"covs\", covs)\n",
    "        self.register_buffer(\"weights\", weights)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.means.shape[1]\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MixtureSameFamily(\n",
    "                mixture_distribution=D.Categorical(probs=self.weights, validate_args=False),\n",
    "                component_distribution=D.MultivariateNormal(\n",
    "                    loc=self.means,\n",
    "                    covariance_matrix=self.covs,\n",
    "                    validate_args=False,\n",
    "                ),\n",
    "                validate_args=False,\n",
    "            )\n",
    "\n",
    "    def log_density(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.distribution.log_prob(x).view(-1, 1)\n",
    "\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return self.distribution.sample(torch.Size((num_samples,)))\n",
    "\n",
    "    @classmethod\n",
    "    def random_2D(\n",
    "        cls, nmodes: int, std: float, scale: float = 10.0, seed = 0.0\n",
    "    ) -> \"GaussianMixture\":\n",
    "        torch.manual_seed(seed)\n",
    "        means = (torch.rand(nmodes, 2) - 0.5) * scale\n",
    "        covs = torch.diag_embed(torch.ones(nmodes, 2)) * std ** 2\n",
    "        weights = torch.ones(nmodes)\n",
    "        return cls(means, covs, weights)\n",
    "\n",
    "    @classmethod\n",
    "    def symmetric_2D(\n",
    "        cls, nmodes: int, std: float, scale: float = 10.0,\n",
    "    ) -> \"GaussianMixture\":\n",
    "        angles = torch.linspace(0, 2 * np.pi, nmodes + 1)[:nmodes]\n",
    "        means = torch.stack([torch.cos(angles), torch.sin(angles)], dim=1) * scale\n",
    "        covs = torch.diag_embed(torch.ones(nmodes, 2) * std ** 2)\n",
    "        weights = torch.ones(nmodes) / nmodes\n",
    "        return cls(means, covs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce6533-2e56-42cb-98e7-958c45d583f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize densities\n",
    "densities = {\n",
    "    \"Gaussian\": Gaussian(mean=torch.zeros(2), cov=10 * torch.eye(2)).to(device),\n",
    "    \"Random Mixture\": GaussianMixture.random_2D(nmodes=5, std=1.0, scale=20.0, seed=3.0).to(device),\n",
    "    \"Symmetric Mixture\": GaussianMixture.symmetric_2D(nmodes=5, std=1.0, scale=8.0).to(device),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(18, 6))\n",
    "bins = 100\n",
    "scale = 15\n",
    "for idx, (name, density) in enumerate(densities.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.set_title(name)\n",
    "    imshow_density(density, bins, scale, ax, vmin=-15, cmap=plt.get_cmap('Blues'))\n",
    "    contour_density(density, bins, scale, ax, colors='grey', linestyles='solid', alpha=0.25, levels=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51093e-b25a-4cdb-bc0d-ab8c1a3145b3",
   "metadata": {},
   "source": [
    "### Question 3.1: Implementing Langevin Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91888056-b900-401b-bebb-4e4bf5301afe",
   "metadata": {},
   "source": [
    "In this section, we'll simulate the (overdamped) Langevin dynamics $$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma dW_t.$$\n",
    "\n",
    "**Your job**: Fill in the `drift_coefficient` and `diffusion_coefficient` methods of the class `LangevinSDE` below.\n",
    "\n",
    "**Hint**: Use `Density.score` to access the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f00c5-3030-4f6c-945f-4a7d3483ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for LangevinSDE\n",
    "def test_langevin_sde():\n",
    "    \"\"\"Test the Langevin SDE implementation\"\"\"\n",
    "    print(\"Testing LangevinSDE...\")\n",
    "    \n",
    "    # Test 1: Basic functionality with Gaussian density\n",
    "    sigma = 1.0\n",
    "    mean = torch.zeros(2).to(device)\n",
    "    cov = torch.eye(2).to(device)\n",
    "    gaussian = Gaussian(mean, cov).to(device)\n",
    "    langevin = LangevinSDE(sigma, gaussian)\n",
    "    \n",
    "    xt = torch.tensor([[1.0, 0.0], [0.0, 1.0]]).to(device)\n",
    "    t = torch.tensor(0.5).to(device)\n",
    "    \n",
    "    drift = langevin.drift_coefficient(xt, t)\n",
    "    diffusion = langevin.diffusion_coefficient(xt, t)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert drift.shape == xt.shape, f\"Drift shape mismatch: {drift.shape} vs {xt.shape}\"\n",
    "    assert diffusion.shape == xt.shape, f\"Diffusion shape mismatch\"\n",
    "    print(\"  ✓ Test 1 passed: Basic shape consistency\")\n",
    "    \n",
    "    # Test 2: Diffusion coefficient should be sigma\n",
    "    expected_diffusion = torch.ones_like(xt) * sigma\n",
    "    assert torch.allclose(diffusion, expected_diffusion, atol=1e-6), \\\n",
    "        f\"Diffusion should be {sigma}, got {diffusion}\"\n",
    "    print(\"  ✓ Test 2 passed: Diffusion coefficient is sigma\")\n",
    "    \n",
    "    # Test 3: Drift at mean should be zero (for Gaussian centered at origin)\n",
    "    xt_mean = torch.zeros(5, 2).to(device)\n",
    "    drift_at_mean = langevin.drift_coefficient(xt_mean, t)\n",
    "    assert torch.allclose(drift_at_mean, torch.zeros_like(xt_mean), atol=1e-4), \\\n",
    "        f\"Drift at mean should be ~0, got {drift_at_mean}\"\n",
    "    print(\"  ✓ Test 3 passed: Drift at mean is zero for centered Gaussian\")\n",
    "    \n",
    "    # Test 4: Drift should point toward mean (for points away from mean)\n",
    "    # For standard Gaussian, score = -x, so drift = 0.5 * sigma^2 * (-x)\n",
    "    xt_away = torch.tensor([[5.0, 0.0], [0.0, 5.0]]).to(device)\n",
    "    drift_away = langevin.drift_coefficient(xt_away, t)\n",
    "    \n",
    "    # Drift should point toward origin (negative direction)\n",
    "    expected_sign = -(0.5 * sigma**2) * xt_away\n",
    "    assert torch.allclose(drift_away, expected_sign, atol=0.1), \\\n",
    "        f\"Drift should point toward mean\"\n",
    "    print(\"  ✓ Test 4 passed: Drift points toward mean\")\n",
    "    \n",
    "    # Test 5: Drift formula correctness\n",
    "    # For N(0, I), score = -x, drift = 0.5 * sigma^2 * (-x)\n",
    "    xt_test = torch.tensor([[2.0, 3.0]]).to(device)\n",
    "    drift_test = langevin.drift_coefficient(xt_test, t)\n",
    "    score = gaussian.score(xt_test)\n",
    "    expected_drift = 0.5 * sigma**2 * score\n",
    "    assert torch.allclose(drift_test, expected_drift, atol=1e-4), \\\n",
    "        f\"Drift formula incorrect. Expected {expected_drift}, got {drift_test}\"\n",
    "    print(\"  ✓ Test 5 passed: Drift formula (0.5 * sigma^2 * score)\")\n",
    "    \n",
    "    # Test 6: Different sigma values\n",
    "    sigmas = [0.5, 1.0, 2.0, 5.0]\n",
    "    xt = torch.tensor([[1.0, 1.0]]).to(device)\n",
    "    for sig in sigmas:\n",
    "        langevin = LangevinSDE(sig, gaussian)\n",
    "        diffusion = langevin.diffusion_coefficient(xt, t)\n",
    "        expected = torch.ones_like(xt) * sig\n",
    "        assert torch.allclose(diffusion, expected, atol=1e-6), \\\n",
    "            f\"For sigma={sig}, diffusion mismatch\"\n",
    "        \n",
    "        drift = langevin.drift_coefficient(xt, t)\n",
    "        score = gaussian.score(xt)\n",
    "        expected_drift = 0.5 * sig**2 * score\n",
    "        assert torch.allclose(drift, expected_drift, atol=1e-4), \\\n",
    "            f\"For sigma={sig}, drift mismatch\"\n",
    "    print(\"  ✓ Test 6 passed: Different sigma values\")\n",
    "    \n",
    "    # Test 7: Shape consistency across batch sizes\n",
    "    test_shapes = [(1, 2), (10, 2), (100, 2)]\n",
    "    for shape in test_shapes:\n",
    "        xt = torch.randn(*shape).to(device)\n",
    "        drift = langevin.drift_coefficient(xt, t)\n",
    "        diffusion = langevin.diffusion_coefficient(xt, t)\n",
    "        assert drift.shape == shape, f\"Drift shape mismatch for {shape}\"\n",
    "        assert diffusion.shape == shape, f\"Diffusion shape mismatch for {shape}\"\n",
    "    print(\"  ✓ Test 7 passed: Batch size consistency\")\n",
    "    \n",
    "    # Test 8: Time independence (for time-independent density)\n",
    "    xt = torch.randn(5, 2).to(device)\n",
    "    t1 = torch.tensor(0.0).to(device)\n",
    "    t2 = torch.tensor(1.0).to(device)\n",
    "    \n",
    "    drift1 = langevin.drift_coefficient(xt, t1)\n",
    "    drift2 = langevin.drift_coefficient(xt, t2)\n",
    "    diffusion1 = langevin.diffusion_coefficient(xt, t1)\n",
    "    diffusion2 = langevin.diffusion_coefficient(xt, t2)\n",
    "    \n",
    "    assert torch.allclose(drift1, drift2, atol=1e-6), \"Drift should be time-independent\"\n",
    "    assert torch.allclose(diffusion1, diffusion2, atol=1e-6), \\\n",
    "        \"Diffusion should be time-independent\"\n",
    "    print(\"  ✓ Test 8 passed: Time independence\")\n",
    "    \n",
    "    # Test 9: Gaussian mixture density\n",
    "    means = torch.tensor([[3.0, 3.0], [-3.0, -3.0]]).to(device)\n",
    "    covs = torch.stack([torch.eye(2), torch.eye(2)]).to(device) * 0.5\n",
    "    weights = torch.tensor([0.5, 0.5]).to(device)\n",
    "    mixture = GaussianMixture(means, covs, weights).to(device)\n",
    "    \n",
    "    sigma = 1.0\n",
    "    langevin = LangevinSDE(sigma, mixture)\n",
    "    \n",
    "    xt = torch.tensor([[3.0, 3.0]]).to(device)  # At first mode\n",
    "    drift = langevin.drift_coefficient(xt, t)\n",
    "    diffusion = langevin.diffusion_coefficient(xt, t)\n",
    "    \n",
    "    # Should have valid outputs\n",
    "    assert not torch.isnan(drift).any(), \"Drift contains NaN\"\n",
    "    assert not torch.isnan(diffusion).any(), \"Diffusion contains NaN\"\n",
    "    assert drift.shape == xt.shape, \"Shape mismatch for mixture\"\n",
    "    print(\"  ✓ Test 9 passed: Works with Gaussian mixture\")\n",
    "    \n",
    "    # Test 10: Stationary distribution property (statistical test)\n",
    "    # Sample from target, run Langevin, should stay near target distribution\n",
    "    sigma = 0.5\n",
    "    gaussian = Gaussian(torch.zeros(2).to(device), torch.eye(2).to(device)).to(device)\n",
    "    langevin = LangevinSDE(sigma, gaussian)\n",
    "    simulator = EulerMaruyamaSimulator(langevin)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = gaussian.sample(1000)  # Start from target distribution\n",
    "    ts = torch.linspace(0.0, 1.0, 101).to(device)\n",
    "    x_final = simulator.simulate(x0, ts)\n",
    "    \n",
    "    # Should remain near N(0, I)\n",
    "    assert abs(x_final.mean()) < 0.2, f\"Mean drifted too far: {x_final.mean()}\"\n",
    "    assert abs(x_final.var() - 1.0) < 0.3, f\"Variance changed too much: {x_final.var()}\"\n",
    "    print(\"  ✓ Test 10 passed: Preserves stationary distribution\")\n",
    "    \n",
    "    # Test 11: Convergence to target from far away\n",
    "    sigma = 1.0\n",
    "    target = Gaussian(torch.zeros(2).to(device), torch.eye(2).to(device)).to(device)\n",
    "    langevin = LangevinSDE(sigma, target)\n",
    "    simulator = EulerMaruyamaSimulator(langevin)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.ones(500, 2).to(device) * 10  # Start far from target\n",
    "    ts = torch.linspace(0.0, 20.0, 2001).to(device)  # Long simulation\n",
    "    x_final = simulator.simulate(x0, ts)\n",
    "    \n",
    "    # Should converge toward N(0, I)\n",
    "    final_mean = x_final.mean(dim=0)\n",
    "    final_var = x_final.var(dim=0).mean()\n",
    "    \n",
    "    assert torch.norm(final_mean) < 0.5, \\\n",
    "        f\"Mean should converge to 0, got {final_mean}\"\n",
    "    assert abs(final_var - 1.0) < 0.5, \\\n",
    "        f\"Variance should converge to 1, got {final_var}\"\n",
    "    print(\"  ✓ Test 11 passed: Convergence to target distribution\")\n",
    "    \n",
    "    print(\"✅ All LangevinSDE tests passed!\\n\")\n",
    "\n",
    "# Run the tests\n",
    "try:\n",
    "    test_langevin_sde()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed with error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2235a-befd-4f25-b0ac-849c9868cd5d",
   "metadata": {},
   "source": [
    "Now, let's graph the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7347543-533a-48c9-9c08-193898b139c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define two utility functions...\n",
    "def every_nth_index(num_timesteps: int, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the indices to record in the trajectory\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return torch.arange(num_timesteps)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.arange(0, num_timesteps - 1, n),\n",
    "            torch.tensor([num_timesteps - 1]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def graph_dynamics(\n",
    "    num_samples: int,\n",
    "    source_distribution: Sampleable,\n",
    "    simulator: Simulator, \n",
    "    density: Density,\n",
    "    timesteps: torch.Tensor, \n",
    "    plot_every: int,\n",
    "    bins: int,\n",
    "    scale: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the evolution of samples from source under the simulation scheme given by simulator (itself a discretization of an ODE or SDE).\n",
    "    Args:\n",
    "        - num_samples: the number of samples to simulate\n",
    "        - source_distribution: distribution from which we draw initial samples at t=0\n",
    "        - simulator: the discertized simulation scheme used to simulate the dynamics\n",
    "        - density: the target density\n",
    "        - timesteps: the timesteps used by the simulator\n",
    "        - plot_every: number of timesteps between consecutive plots\n",
    "        - bins: number of bins for imshow\n",
    "        - scale: scale for imshow\n",
    "    \"\"\"\n",
    "    # Simulate\n",
    "    x0 = source_distribution.sample(num_samples)\n",
    "    xts = simulator.simulate_with_trajectory(x0, timesteps)\n",
    "    indices_to_plot = every_nth_index(len(timesteps), plot_every)\n",
    "    plot_timesteps = timesteps[indices_to_plot]\n",
    "    plot_xts = xts[:,indices_to_plot]\n",
    "\n",
    "    # Graph\n",
    "    fig, axes = plt.subplots(2, len(plot_timesteps), figsize=(8*len(plot_timesteps), 16))\n",
    "    axes = axes.reshape((2,len(plot_timesteps)))\n",
    "    for t_idx in range(len(plot_timesteps)):\n",
    "        t = plot_timesteps[t_idx].item()\n",
    "        xt = plot_xts[:,t_idx]\n",
    "        # Scatter axes\n",
    "        scatter_ax = axes[0, t_idx]\n",
    "        imshow_density(density, bins, scale, scatter_ax, vmin=-15, alpha=0.25, cmap=plt.get_cmap('Blues'))\n",
    "        scatter_ax.scatter(xt[:,0].cpu(), xt[:,1].cpu(), marker='x', color='black', alpha=0.75, s=15)\n",
    "        scatter_ax.set_title(f'Samples at t={t:.1f}', fontsize=15)\n",
    "        scatter_ax.set_xticks([])\n",
    "        scatter_ax.set_yticks([])\n",
    "\n",
    "        # Kdeplot axes\n",
    "        kdeplot_ax = axes[1, t_idx]\n",
    "        imshow_density(density, bins, scale, kdeplot_ax, vmin=-15, alpha=0.5, cmap=plt.get_cmap('Blues'))\n",
    "        sns.kdeplot(x=xt[:,0].cpu(), y=xt[:,1].cpu(), alpha=0.5, ax=kdeplot_ax,color='grey')\n",
    "        kdeplot_ax.set_title(f'Density of Samples at t={t:.1f}', fontsize=15)\n",
    "        kdeplot_ax.set_xticks([])\n",
    "        kdeplot_ax.set_yticks([])\n",
    "        kdeplot_ax.set_xlabel(\"\")\n",
    "        kdeplot_ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b244a6-5b25-4b83-a4fb-ff14c29d5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the simulator\n",
    "target = GaussianMixture.random_2D(nmodes=5, std=0.75, scale=15.0, seed=3.0).to(device)\n",
    "sde = LangevinSDE(sigma = 0.6, density = target)\n",
    "simulator = EulerMaruyamaSimulator(sde)\n",
    "\n",
    "# Graph the results!\n",
    "graph_dynamics(\n",
    "    num_samples = 1000,\n",
    "    source_distribution = Gaussian(mean=torch.zeros(2), cov=20 * torch.eye(2)).to(device),\n",
    "    simulator=simulator,\n",
    "    density=target,\n",
    "    timesteps=torch.linspace(0,5.0,1000).to(device),\n",
    "    plot_every=334,\n",
    "    bins=200,\n",
    "    scale=15\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d75ef-806a-4012-ae59-6cc7faa5eaf1",
   "metadata": {},
   "source": [
    "**Your job**: Try varying the value of $\\sigma$, the number and range of the simulation steps, the source distribution, and target density. What do you notice? Why?\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d552e-0d1d-4cae-b5e6-23d270b7193f",
   "metadata": {},
   "source": [
    "Note: To run the folowing two **optional** cells, you will need to download the `ffmpeg` library. You can do so using e.g., `conda install -c conda-forge ffmpeg` (or, ideally, `mamba`). Running `pip install ffmpeg` or similar will likely **not** work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a53ea-d24e-4113-a190-e53d661bacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "def animate_dynamics(\n",
    "    num_samples: int,\n",
    "    source_distribution: Sampleable,\n",
    "    simulator: Simulator, \n",
    "    density: Density,\n",
    "    timesteps: torch.Tensor, \n",
    "    animate_every: int,\n",
    "    bins: int,\n",
    "    scale: float,\n",
    "    save_path: str = 'dynamics_animation.mp4'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the evolution of samples from source under the simulation scheme given by simulator (itself a discretization of an ODE or SDE).\n",
    "    Args:\n",
    "        - num_samples: the number of samples to simulate\n",
    "        - source_distribution: distribution from which we draw initial samples at t=0\n",
    "        - simulator: the discertized simulation scheme used to simulate the dynamics\n",
    "        - density: the target density\n",
    "        - timesteps: the timesteps used by the simulator\n",
    "        - animate_every: number of timesteps between consecutive frames in the resulting animation\n",
    "    \"\"\"\n",
    "    # Simulate\n",
    "    x0 = source_distribution.sample(num_samples)\n",
    "    xts = simulator.simulate_with_trajectory(x0, timesteps)\n",
    "    indices_to_animate = every_nth_index(len(timesteps), animate_every)\n",
    "    animate_timesteps = timesteps[indices_to_animate]\n",
    "    animate_xts = xts[:, indices_to_animate]\n",
    "\n",
    "    # Graph\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    camera = Camera(fig)\n",
    "    for t_idx in range(len(animate_timesteps)):\n",
    "        t = animate_timesteps[t_idx].item()\n",
    "        xt = animate_xts[:,t_idx]\n",
    "        # Scatter axes\n",
    "        scatter_ax = axes[0]\n",
    "        imshow_density(density, bins, scale, scatter_ax, vmin=-15, alpha=0.25, cmap=plt.get_cmap('Blues'))\n",
    "        scatter_ax.scatter(xt[:,0].cpu(), xt[:,1].cpu(), marker='x', color='black', alpha=0.75, s=15)\n",
    "        scatter_ax.set_title(f'Samples')\n",
    "\n",
    "        # Kdeplot axes\n",
    "        kdeplot_ax = axes[1]\n",
    "        imshow_density(density, bins, scale, kdeplot_ax, vmin=-15, alpha=0.5, cmap=plt.get_cmap('Blues'))\n",
    "        sns.kdeplot(x=xt[:,0].cpu(), y=xt[:,1].cpu(), alpha=0.5, ax=kdeplot_ax,color='grey')\n",
    "        kdeplot_ax.set_title(f'Density of Samples', fontsize=15)\n",
    "        kdeplot_ax.set_xticks([])\n",
    "        kdeplot_ax.set_yticks([])\n",
    "        kdeplot_ax.set_xlabel(\"\")\n",
    "        kdeplot_ax.set_ylabel(\"\")\n",
    "        camera.snap()\n",
    "    \n",
    "    animation = camera.animate()\n",
    "    animation.save(save_path)\n",
    "    plt.close()\n",
    "    return HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4269e-9e6d-4a50-8353-3b380eae4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CELL\n",
    "# Construct the simulator\n",
    "target = GaussianMixture.random_2D(nmodes=5, std=0.75, scale=15.0, seed=3.0).to(device)\n",
    "sde = LangevinSDE(sigma = 0.6, density = target)\n",
    "simulator = EulerMaruyamaSimulator(sde)\n",
    "\n",
    "# Graph the results!\n",
    "animate_dynamics(\n",
    "    num_samples = 1000,\n",
    "    source_distribution = Gaussian(mean=torch.zeros(2), cov=20 * torch.eye(2)).to(device),\n",
    "    simulator=simulator,\n",
    "    density=target,\n",
    "    timesteps=torch.linspace(0,5.0,1000).to(device),\n",
    "    bins=200,\n",
    "    scale=15,\n",
    "    animate_every=100\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149c323-3c9e-40a7-8e00-15fe8b87f3f8",
   "metadata": {},
   "source": [
    "### Question 3.2: Ornstein-Uhlenbeck as Langevin Dynamics\n",
    "In this section, we'll finish off with a brief mathematical exercise connecting Langevin dynamics and Ornstein-Uhlenbeck processes. Recall that for (suitably nice) distribution $p$, the *Langevin dynamics* are given by\n",
    "$$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0,$$\n",
    "while for given $\\theta, \\sigma$, the Ornstein-Uhlenbeck process is given by\n",
    "$$dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86954c67-510b-4d10-aea1-b5636f4dbb47",
   "metadata": {},
   "source": [
    "**Your job**: Show that when $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$, the score is given by $$\\nabla \\log p(x) = -\\frac{2\\theta}{\\sigma^2}x.$$\n",
    "\n",
    "**Hint**: The probability density of the Gaussian $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$ is given by $$p(x)  = \\frac{\\sqrt{\\theta}}{\\sigma\\sqrt{\\pi}} \\exp\\left(-\\frac{x^2\\theta}{\\sigma^2}\\right).$$\n",
    "\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0f761-c85c-4e30-b497-bc7622bc8e72",
   "metadata": {},
   "source": [
    "**Your job**: Conclude that when $p(x) = N(0, \\frac{\\sigma^2}{2\\theta})$, the Langevin dynamics \n",
    "$$dX_t = \\frac{1}{2} \\sigma^2\\nabla \\log p(X_t) dt + \\sigma dW_t,$$\n",
    "is equivalent to the Ornstein-Uhlenbeck process\n",
    "$$ dX_t = -\\theta X_t\\, dt + \\sigma\\, dW_t, \\quad \\quad X_0 = 0.$$\n",
    "\n",
    "**Your answer**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
